{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp0pcBxsVAMy"
      },
      "source": [
        "___\n",
        "\n",
        "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxOaSxtJWV1G"
      },
      "source": [
        "# WELCOME\n",
        "\n",
        "This notebook will guide you through two increasingly significant applications in the realm of Generative AI: RAG (Retrieval Augmented Generation) chatbots and text summarization for big text.\n",
        "\n",
        "Through two distinct projects, you will explore these technologies and enhance your skills. Detailed descriptions of the projects are provided below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaCz7nhxKI9R"
      },
      "source": [
        "## Project 1: Building a Chatbot with a PDF Document (RAG)\n",
        "\n",
        "In this project, you will develop a chatbot using a provided PDF document from web page. You will utilize the Langchain framework along with a large language model (LLM) such as GPT or Gemini. The chatbot will leverage the Retrieval Augmented Generation (RAG) technique to comprehend the document's content and respond to user queries effectively.\n",
        "\n",
        "### **Project Steps:**\n",
        "\n",
        "- **1.PDF Document Upload:** Upload the provided PDF document from web page (https://aclanthology.org/N19-1423.pdf) (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding).\n",
        "\n",
        "- **2.Chunking:** Divide the uploaded PDF document into smaller segments (chunks). This facilitates more efficient information processing by the LLM.\n",
        "\n",
        "- **3.ChromaDB Setup:**\n",
        "  - Save ChromaDB to your Google Drive.\n",
        "\n",
        "  - Retrieve ChromaDB from your Drive to begin using it in your project.\n",
        "\n",
        "  - ChromaDB serves as a vector database to store embedding vectors generated from your document.\n",
        "\n",
        "- **4.Embedding Vectors Creation:**\n",
        "  - Convert the chunked document into embedding vectors. You can use either GPT or Gemini embedding models for this purpose.\n",
        "\n",
        "  - If you choose the Gemini embedding model, set \"task_type\" to \"retrieval_document\" when converting the chunked document.\n",
        "\n",
        "- **5.Chatbot Development:**\n",
        "  - Utilize the **load_qa_chain** function from the Langchain library to build the chatbot.\n",
        "\n",
        "  - This function will interpret user queries, retrieve relevant information from **ChromaDB**, and generate responses accordingly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eoQWi-uN0dx"
      },
      "source": [
        "### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCbI4MuNanVu",
        "outputId": "11d26976-31c2-4f7d-e059-00086ca4c878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.15-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.35 (from langchain)\n",
            "  Downloading langchain_core-0.2.37-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.108-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.35->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m539.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.2.15-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.37-py3-none-any.whl (396 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.2/396.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.108-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.15 langchain-core-0.2.37 langchain-text-splitters-0.2.2 langsmith-0.1.108 orjson-3.10.7 tenacity-8.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOaahY-AancA",
        "outputId": "23cd651b-8140-47f4-9d6d-10f1a641fce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m877.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8Emaqo6MRmY",
        "outputId": "365fde4c-152d-4227-9961-fc98ebc09af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-google-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9LlHMX-yNJph"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siJCXqNhNQKu",
        "outputId": "8e7bc640-2d10-4bab-b394-fa57117dbef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCDOhYABNUBa",
        "outputId": "09203145-d48e-4fcc-80c9-762e84a73e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m770.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU pypdfium2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuLllnCl2yfe"
      },
      "source": [
        "### Access Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQR06EhDapPP",
        "outputId": "b4cf026d-4266-4259-cc1e-ab80c78adca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uR9bJp_0MyF"
      },
      "source": [
        "### Entering Your OpenAI or Google Gemini API Key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2jwo1SQ2asnZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('OPENAI_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV9rG-0PN8p0"
      },
      "source": [
        "### Loading PDF Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "5H6eQyYyauxP"
      },
      "outputs": [],
      "source": [
        "# create a pdf reader function\n",
        "from langchain.document_loaders import PyPDFium2Loader\n",
        "\n",
        "def read_doc(directory):\n",
        "    file_loader=PyPDFium2Loader(directory)\n",
        "    pdf_documents=file_loader.load() # PyPDFium2Loader reads page by page\n",
        "    return pdf_documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_kXJZ5Taupv",
        "outputId": "0c35ebeb-8710-4adc-e990-ea292d508214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
            "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "pdf=read_doc('/content/drive/MyDrive/N19-1423.pdf')\n",
        "len(pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLQ1j_JrOF57"
      },
      "source": [
        "### Document Splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "HHQlclU9awwa"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "\n",
        "def chunk_data(docs, chunk_size=1000, chunk_overlap=200):\n",
        "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
        "                                                 chunk_overlap=chunk_overlap)\n",
        "    pdf=text_splitter.split_documents(docs)\n",
        "    return pdf\n",
        "\n",
        "# This code splits documents into chunks using the RecursiveCharacterTextSplitter class from the langchain library.\n",
        "\n",
        "# A function named chunk_data is defined, which takes a document or a collection of documents (docs) as input.\n",
        "# It also takes two parameters: chunk_size and chunk_overlap.\n",
        "# chunk_size specifies the maximum number of characters in each chunk, while chunk_overlap determines the amount of overlap between consecutive chunks.\n",
        "\n",
        "# The function divides the documents into chunks based on these parameters using the RecursiveCharacterTextSplitter class.\n",
        "# Consequently, each chunk contains chunk_size characters, with an overlap of chunk_overlap characters between consecutive chunks.\n",
        "\n",
        "# As a result, the documents are segmented into chunks of specified sizes, and these chunks are returned.\n",
        "\n",
        "# The chunk_overlap parameter is used to specify the sharing of characters between consecutive chunks.\n",
        "# In other words, it ensures that the characters at the end of one chunk reappear at the beginning of the next chunk.\n",
        "# This prevents the loss of information when the text is segmented or divided and helps preserve a certain context.\n",
        "# Especially, overlap can be used to maintain important contextual relationships within a specific text and sustain meaning across chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaQV6XRwawpf",
        "outputId": "b9686013-dff7-44d2-b059-3fab927777a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "pdf_doc=chunk_data(docs=pdf)\n",
        "len(pdf_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt5gt_BNTWlY",
        "outputId": "29e69614-1fa5-4ce9-e092-0454a9df370f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/drive/MyDrive/N19-1423.pdf', 'page': 2}, page_content='H, and the number of self-attention heads as A.\\r\\n3\\r\\nWe primarily report results on two model sizes:\\r\\nBERTBASE (L=12, H=768, A=12, Total Param\\x02eters=110M) and BERTLARGE (L=24, H=1024,\\r\\nA=16, Total Parameters=340M).\\r\\nBERTBASE was chosen to have the same model\\r\\nsize as OpenAI GPT for comparison purposes.\\r\\nCritically, however, the BERT Transformer uses\\r\\nbidirectional self-attention, while the GPT Trans\\x02former uses constrained self-attention where every\\r\\ntoken can only attend to context to its left.4\\r\\n1\\r\\nhttps://github.com/tensorflow/tensor2tensor\\r\\n2\\r\\nhttp://nlp.seas.harvard.edu/2018/04/03/attention.html\\r\\n3\\r\\nIn all cases we set the feed-forward/filter size to be 4H,\\r\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\r\\n4We note that in the literature the bidirectional Trans-'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/N19-1423.pdf', 'page': 3}, page_content='4174\\r\\nInput/Output Representations To make BERT\\r\\nhandle a variety of down-stream tasks, our input\\r\\nrepresentation is able to unambiguously represent\\r\\nboth a single sentence and a pair of sentences\\r\\n(e.g., h Question, Answeri) in one token sequence.\\r\\nThroughout this work, a “sentence” can be an arbi\\x02trary span of contiguous text, rather than an actual\\r\\nlinguistic sentence. A “sequence” refers to the in\\x02put token sequence to BERT, which may be a sin\\x02gle sentence or two sentences packed together.\\r\\nWe use WordPiece embeddings (Wu et al.,\\r\\n2016) with a 30,000 token vocabulary. The first\\r\\ntoken of every sequence is always a special clas\\x02sification token ([CLS]). The final hidden state\\r\\ncorresponding to this token is used as the ag\\x02gregate sequence representation for classification\\r\\ntasks. Sentence pairs are packed together into a\\r\\nsingle sequence. We differentiate the sentences in\\r\\ntwo ways. First, we separate them with a special')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "pdf_doc[15:17]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ENim_5MOT9O"
      },
      "source": [
        "### 1. Creating An Embedding Model\n",
        "### 2. Convert the Each Chunk of The Split Document to Embedding Vectors\n",
        "### 3. Storing of The Embedding Vectors to Vectorstore\n",
        "### 4. Save the Vectorstore to Your Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96nLFF1ja0k_",
        "outputId": "df48d5e5-ed04-47a7-94a3-b574559940f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7d10b42a0310>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7d10a08d67d0>, model='text-embedding-3-large', dimensions=3072, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings=OpenAIEmbeddings(model=\"text-embedding-3-large\",\n",
        "                            dimensions=3072)   # generated embedding vectors with dimension 3072\n",
        "embeddings\n",
        "\n",
        "# As the embedding model, we use Openai's latest introduced text-embedding-3-large model.\n",
        "# dimensions of text-embedding-3-large are 256, 1024 and 3072\n",
        "# dimensions of text-embedding-3-small are 512 and 1536\n",
        "# dimension of text-embedding-ada-002 is only 1536\n",
        "# text-embedding-3-large gives the best embedding performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VqZ7XBwoa0ee"
      },
      "outputs": [],
      "source": [
        "text = \"This is a proje document.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "otrixMa1aKRP"
      },
      "outputs": [],
      "source": [
        "doc_result = embeddings.embed_documents([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-EdqI3vaNEj",
        "outputId": "1e12f9f1-6a90-4c41-e010-9a83afb2ef0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.012167209759354591,\n",
              " -0.014666931703686714,\n",
              " -0.030242852866649628,\n",
              " 0.03153058886528015,\n",
              " -0.01176952663809061]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "doc_result[0][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK1IjsAPaRbe",
        "outputId": "9d3ceebf-bccd-4074-b752-a150b37074b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3072"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(doc_result[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2tMqUthPchD"
      },
      "source": [
        "### Load Vectorstore(index) From Your Drive or Create New Vectorstore Directory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore_dir = \"/content/drive/MyDrive/vectorstore\"\n",
        "\n",
        "from langchain_chroma import Chroma"
      ],
      "metadata": {
        "id": "Mdgma0UjE5o2"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load or create Chroma index\n",
        "def load_or_create_chroma(pdf_chunks, vectorstore_dir):\n",
        "    # Initialize OpenAI embeddings\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=3072, openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
        "\n",
        "    # Check if the vectorstore directory exists\n",
        "    if os.path.exists(vectorstore_dir):\n",
        "        # Load existing Chroma index\n",
        "        loaded_index = Chroma(persist_directory=vectorstore_dir, embedding_function=embeddings)\n",
        "        print(\"Loaded existing Chroma index from the directory.\")\n",
        "        load_retriever = loaded_index.as_retriever(search_kwargs={\"k\": 5})\n",
        "        return loaded_index, load_retriever\n",
        "    else:\n",
        "        # Create new Chroma index from document chunks\n",
        "        index = Chroma.from_documents(documents=pdf_chunks, embedding=embeddings, persist_directory=vectorstore_dir)\n",
        "        print(\"Created a new Chroma index and saved it to the directory.\")\n",
        "        retriever = index.as_retriever(search_kwargs={\"k\": 5})\n",
        "        return index, retriever\n",
        "\n",
        "# Load or create the Chroma index\n",
        "index, retriever = load_or_create_chroma(pdf_doc, vectorstore_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfpkAyFTGUJ4",
        "outputId": "8022c672-0de0-4db4-a1cf-633639701f0c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing Chroma index from the directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFxCABagb4eX",
        "outputId": "27baf7e2-418f-4f1a-b7ec-d1eb9316b85f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='to create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan'),\n",
              " Document(metadata={'page': 2, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='4173\\r\\nBERT BERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nQuestion Paragraph\\r\\nStart/End Span\\r\\nBERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nMasked Sentence A Masked Sentence B\\r\\nPre-training Fine-Tuning\\r\\nNSP Mask LM Mask LM\\r\\nUnlabeled Sentence A and B Pair \\r\\nSQuAD\\r\\nQuestion Answer Pair\\r\\nMNLI NER\\r\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\r\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\r\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\\x02tions/answers).\\r\\ning and auto-encoder objectives have been used'),\n",
              " Document(metadata={'page': 3, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='4174\\r\\nInput/Output Representations To make BERT\\r\\nhandle a variety of down-stream tasks, our input\\r\\nrepresentation is able to unambiguously represent\\r\\nboth a single sentence and a pair of sentences\\r\\n(e.g., h Question, Answeri) in one token sequence.\\r\\nThroughout this work, a “sentence” can be an arbi\\x02trary span of contiguous text, rather than an actual\\r\\nlinguistic sentence. A “sequence” refers to the in\\x02put token sequence to BERT, which may be a sin\\x02gle sentence or two sentences packed together.\\r\\nWe use WordPiece embeddings (Wu et al.,\\r\\n2016) with a 30,000 token vocabulary. The first\\r\\ntoken of every sequence is always a special clas\\x02sification token ([CLS]). The final hidden state\\r\\ncorresponding to this token is used as the ag\\x02gregate sequence representation for classification\\r\\ntasks. Sentence pairs are packed together into a\\r\\nsingle sequence. We differentiate the sentences in\\r\\ntwo ways. First, we separate them with a special'),\n",
              " Document(metadata={'page': 12, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='4183\\r\\nBERT (Ours)\\r\\nTrm Trm Trm\\r\\nTrm Trm Trm\\r\\n...\\r\\n...\\r\\nTrm Trm Trm\\r\\nTrm Trm Trm\\r\\n...\\r\\n...\\r\\nOpenAI GPT\\r\\nLstm\\r\\nELMo\\r\\nLstm Lstm\\r\\nLstm Lstm Lstm\\r\\nLstm Lstm Lstm\\r\\nLstm Lstm Lstm\\r\\n T1 T2\\r\\n TN\\r\\n...\\r\\n...\\r\\n...\\r\\n...\\r\\n...\\r\\n E1 E2\\r\\n EN\\r\\n...\\r\\n T1 T2 TN\\r\\n...\\r\\n E1 E2\\r\\n E ... N\\r\\n T1 T2\\r\\n TN\\r\\n...\\r\\n E1 E2\\r\\n EN\\r\\n...\\r\\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\\r\\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to\\x02left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\\r\\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\\r\\nOpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.\\r\\nto converge. In Section C.1 we demonstrate that\\r\\nMLM does converge marginally slower than a left\\x02to-right model (which predicts every token), but'),\n",
              " Document(metadata={'page': 2, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='A distinctive feature of BERT is its unified ar\\x02chitecture across different tasks. There is mini\\x02mal difference between the pre-trained architec\\x02ture and the final downstream architecture.\\r\\nModel Architecture BERT’s model architec\\x02ture is a multi-layer bidirectional Transformer en\\x02coder based on the original implementation de\\x02scribed in Vaswani et al. (2017) and released in\\r\\nthe tensor2tensor library.1 Because the use\\r\\nof Transformers has become common and our im\\x02plementation is almost identical to the original,\\r\\nwe will omit an exhaustive background descrip\\x02tion of the model architecture and refer readers to\\r\\nVaswani et al. (2017) as well as excellent guides\\r\\nsuch as “The Annotated Transformer.”2\\r\\nIn this work, we denote the number of layers\\r\\n(i.e., Transformer blocks) as L, the hidden size as\\r\\nH, and the number of self-attention heads as A.\\r\\n3\\r\\nWe primarily report results on two model sizes:\\r\\nBERTBASE (L=12, H=768, A=12, Total Param\\x02eters=110M) and BERTLARGE (L=24, H=1024,')]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "retriever.invoke(\"What is Bert?\")\n",
        "\n",
        "# retriever.invoke first converts text to embedding vector with embedding algorithm\n",
        "# Then looks at the similarity scores with each of the 84 pieces and ranks them\n",
        "# Since k=5, it takes the 5 most similar vectors and returns the original texts corresponding to these vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKA0PgNJQOmj"
      },
      "source": [
        "### Retrival the First 5 Chunks That Are Most Similar to The User Query from The Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "80TWdFk6a4-3"
      },
      "outputs": [],
      "source": [
        "def retrieve_query(query,k=5):\n",
        "    retriever=index.as_retriever(search_kwargs={\"k\": k}) #loaded_index\n",
        "    return retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What is Fine-Tuning ?\"\n",
        "\n",
        "doc_search=retrieve_query(our_query, k=5) # first five most similar texts are returned\n",
        "doc_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORhL_ct5La05",
        "outputId": "586d224b-55cc-4e2d-8766-e38195333ee5"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 4, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='answering, and the [CLS] representation is fed\\r\\ninto an output layer for classification, such as en\\x02tailment or sentiment analysis.\\r\\nCompared to pre-training, fine-tuning is rela\\x02tively inexpensive. All of the results in the pa\\x02per can be replicated in at most 1 hour on a sin\\x02gle Cloud TPU, or a few hours on a GPU, starting\\r\\nfrom the exact same pre-trained model.7 We de\\x02scribe the task-specific details in the correspond\\x02ing subsections of Section 4. More details can be\\r\\nfound in Appendix A.5.\\r\\n4 Experiments\\r\\nIn this section, we present BERT fine-tuning re\\x02sults on 11 NLP tasks.\\r\\n4.1 GLUE\\r\\nThe General Language Understanding Evaluation\\r\\n(GLUE) benchmark (Wang et al., 2018a) is a col\\x02lection of diverse natural language understanding\\r\\ntasks. Detailed descriptions of GLUE datasets are\\r\\nincluded in Appendix B.1.\\r\\nTo fine-tune on GLUE, we represent the input\\r\\nsequence (for single sentence or sentence pairs)\\r\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R'),\n",
              " Document(metadata={'page': 4, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='included in Appendix B.1.\\r\\nTo fine-tune on GLUE, we represent the input\\r\\nsequence (for single sentence or sentence pairs)\\r\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R\\r\\nH corresponding to the first\\r\\ninput token ([CLS]) as the aggregate representa\\x02tion. The only new parameters introduced during\\r\\nfine-tuning are classification layer weights W ∈\\r\\nR\\r\\nK×H, where K is the number of labels. We com\\x02pute a standard classification loss with C and W,\\r\\ni.e., log(softmax(CWT)).\\r\\n7\\r\\nFor example, the BERT SQuAD model can be trained in\\r\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\r\\nF1 score of 91.0%.\\r\\n8\\r\\nSee (10) in https://gluebenchmark.com/faq.'),\n",
              " Document(metadata={'page': 13, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='[SEP], [CLS] and sentence A/B embed\\x02dings during pre-training.\\r\\n• GPT was trained for 1M steps with a batch\\r\\nsize of 32,000 words; BERT was trained for\\r\\n1M steps with a batch size of 128,000 words.\\r\\n• GPT used the same learning rate of 5e-5 for\\r\\nall fine-tuning experiments; BERT chooses a\\r\\ntask-specific fine-tuning learning rate which\\r\\nperforms the best on the development set.\\r\\nTo isolate the effect of these differences, we per\\x02form ablation experiments in Section 5.1 which\\r\\ndemonstrate that the majority of the improvements\\r\\nare in fact coming from the two pre-training tasks\\r\\nand the bidirectionality they enable.\\r\\nA.5 Illustrations of Fine-tuning on Different\\r\\nTasks\\r\\nThe illustration of fine-tuning BERT on different\\r\\ntasks can be seen in Figure 4. Our task-specific\\r\\nmodels are formed by incorporating BERT with\\r\\none additional output layer, so a minimal num\\x02ber of parameters need to be learned from scratch.\\r\\nAmong the tasks, (a) and (b) are sequence-level'),\n",
              " Document(metadata={'page': 12, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='positional embeddings.\\r\\nA.3 Fine-tuning Procedure\\r\\nFor fine-tuning, most model hyperparameters are\\r\\nthe same as in pre-training, with the exception of\\r\\nthe batch size, learning rate, and number of train\\x02ing epochs. The dropout probability was always\\r\\nkept at 0.1. The optimal hyperparameter values\\r\\nare task-specific, but we found the following range\\r\\nof possible values to work well across all tasks:\\r\\n• Batch size: 16, 32\\r\\n13https://cloudplatform.googleblog.com/2018/06/Cloud\\x02TPU-now-offers-preemptible-pricing-and-global\\x02availability.html'),\n",
              " Document(metadata={'page': 4, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='model pre-training. For the pre-training corpus we\\r\\nuse the BooksCorpus (800M words) (Zhu et al.,\\r\\n2015) and English Wikipedia (2,500M words).\\r\\nFor Wikipedia we extract only the text passages\\r\\nand ignore lists, tables, and headers. It is criti\\x02cal to use a document-level corpus rather than a\\r\\nshuffled sentence-level corpus such as the Billion\\r\\nWord Benchmark (Chelba et al., 2013) in order to\\r\\nextract long contiguous sequences.\\r\\n3.2 Fine-tuning BERT\\r\\nFine-tuning is straightforward since the self\\x02attention mechanism in the Transformer al\\x02lows BERT to model many downstream tasks—\\r\\nwhether they involve single text or text pairs—by\\r\\nswapping out the appropriate inputs and outputs.\\r\\nFor applications involving text pairs, a common\\r\\npattern is to independently encode text pairs be\\x02fore applying bidirectional cross attention, such\\r\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\r\\ninstead uses the self-attention mechanism to unify\\r\\nthese two stages, as encoding a concatenated text')]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What is the advantages of Fine-Tuning?\"\n",
        "\n",
        "doc_search=retrieve_query(our_query, k=5) # first five most similar texts are returned\n",
        "doc_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biezW7eyM5K8",
        "outputId": "cb96b816-ac65-4b28-dc09-e0c303fc338d"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 13, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='[SEP], [CLS] and sentence A/B embed\\x02dings during pre-training.\\r\\n• GPT was trained for 1M steps with a batch\\r\\nsize of 32,000 words; BERT was trained for\\r\\n1M steps with a batch size of 128,000 words.\\r\\n• GPT used the same learning rate of 5e-5 for\\r\\nall fine-tuning experiments; BERT chooses a\\r\\ntask-specific fine-tuning learning rate which\\r\\nperforms the best on the development set.\\r\\nTo isolate the effect of these differences, we per\\x02form ablation experiments in Section 5.1 which\\r\\ndemonstrate that the majority of the improvements\\r\\nare in fact coming from the two pre-training tasks\\r\\nand the bidirectionality they enable.\\r\\nA.5 Illustrations of Fine-tuning on Different\\r\\nTasks\\r\\nThe illustration of fine-tuning BERT on different\\r\\ntasks can be seen in Figure 4. Our task-specific\\r\\nmodels are formed by incorporating BERT with\\r\\none additional output layer, so a minimal num\\x02ber of parameters need to be learned from scratch.\\r\\nAmong the tasks, (a) and (b) are sequence-level'),\n",
              " Document(metadata={'page': 4, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='answering, and the [CLS] representation is fed\\r\\ninto an output layer for classification, such as en\\x02tailment or sentiment analysis.\\r\\nCompared to pre-training, fine-tuning is rela\\x02tively inexpensive. All of the results in the pa\\x02per can be replicated in at most 1 hour on a sin\\x02gle Cloud TPU, or a few hours on a GPU, starting\\r\\nfrom the exact same pre-trained model.7 We de\\x02scribe the task-specific details in the correspond\\x02ing subsections of Section 4. More details can be\\r\\nfound in Appendix A.5.\\r\\n4 Experiments\\r\\nIn this section, we present BERT fine-tuning re\\x02sults on 11 NLP tasks.\\r\\n4.1 GLUE\\r\\nThe General Language Understanding Evaluation\\r\\n(GLUE) benchmark (Wang et al., 2018a) is a col\\x02lection of diverse natural language understanding\\r\\ntasks. Detailed descriptions of GLUE datasets are\\r\\nincluded in Appendix B.1.\\r\\nTo fine-tune on GLUE, we represent the input\\r\\nsequence (for single sentence or sentence pairs)\\r\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R'),\n",
              " Document(metadata={'page': 4, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='included in Appendix B.1.\\r\\nTo fine-tune on GLUE, we represent the input\\r\\nsequence (for single sentence or sentence pairs)\\r\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R\\r\\nH corresponding to the first\\r\\ninput token ([CLS]) as the aggregate representa\\x02tion. The only new parameters introduced during\\r\\nfine-tuning are classification layer weights W ∈\\r\\nR\\r\\nK×H, where K is the number of labels. We com\\x02pute a standard classification loss with C and W,\\r\\ni.e., log(softmax(CWT)).\\r\\n7\\r\\nFor example, the BERT SQuAD model can be trained in\\r\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\r\\nF1 score of 91.0%.\\r\\n8\\r\\nSee (10) in https://gluebenchmark.com/faq.'),\n",
              " Document(metadata={'page': 8, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='all parameters are jointly fine-tuned on a down\\x02stream task. However, the feature-based approach,\\r\\nwhere fixed features are extracted from the pre\\x02trained model, has certain advantages. First, not\\r\\nall tasks can be easily represented by a Trans\\x02former encoder architecture, and therefore require\\r\\na task-specific model architecture to be added.\\r\\nSecond, there are major computational benefits\\r\\nto pre-compute an expensive representation of the\\r\\ntraining data once and then run many experiments\\r\\nwith cheaper models on top of this representation.\\r\\nIn this section, we compare the two approaches\\r\\nby applying BERT to the CoNLL-2003 Named\\r\\nEntity Recognition (NER) task (Tjong Kim Sang\\r\\nand De Meulder, 2003). In the input to BERT, we\\r\\nuse a case-preserving WordPiece model, and we\\r\\ninclude the maximal document context provided\\r\\nby the data. Following standard practice, we for\\x02mulate this as a tagging task but do not use a CRF\\r\\nHyperparams Dev Set Accuracy\\r\\n#L #H #A LM (ppl) MNLI-m MRPC SST-2'),\n",
              " Document(metadata={'page': 12, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='positional embeddings.\\r\\nA.3 Fine-tuning Procedure\\r\\nFor fine-tuning, most model hyperparameters are\\r\\nthe same as in pre-training, with the exception of\\r\\nthe batch size, learning rate, and number of train\\x02ing epochs. The dropout probability was always\\r\\nkept at 0.1. The optimal hyperparameter values\\r\\nare task-specific, but we found the following range\\r\\nof possible values to work well across all tasks:\\r\\n• Batch size: 16, 32\\r\\n13https://cloudplatform.googleblog.com/2018/06/Cloud\\x02TPU-now-offers-preemptible-pricing-and-global\\x02availability.html')]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"How does BERT handle single and paired sentences, and what kind of embeddings and vocabulary size does it use?\"\n",
        "\n",
        "doc_search=retrieve_query(our_query, k=5) # first five most similar texts are returned\n",
        "doc_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsvphfoVLb9x",
        "outputId": "5e61a9a0-1fdf-4fe1-ed4f-c7211eb49cf5"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 3, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='4174\\r\\nInput/Output Representations To make BERT\\r\\nhandle a variety of down-stream tasks, our input\\r\\nrepresentation is able to unambiguously represent\\r\\nboth a single sentence and a pair of sentences\\r\\n(e.g., h Question, Answeri) in one token sequence.\\r\\nThroughout this work, a “sentence” can be an arbi\\x02trary span of contiguous text, rather than an actual\\r\\nlinguistic sentence. A “sequence” refers to the in\\x02put token sequence to BERT, which may be a sin\\x02gle sentence or two sentences packed together.\\r\\nWe use WordPiece embeddings (Wu et al.,\\r\\n2016) with a 30,000 token vocabulary. The first\\r\\ntoken of every sequence is always a special clas\\x02sification token ([CLS]). The final hidden state\\r\\ncorresponding to this token is used as the ag\\x02gregate sequence representation for classification\\r\\ntasks. Sentence pairs are packed together into a\\r\\nsingle sequence. We differentiate the sentences in\\r\\ntwo ways. First, we separate them with a special'),\n",
              " Document(metadata={'page': 3, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='tasks. Sentence pairs are packed together into a\\r\\nsingle sequence. We differentiate the sentences in\\r\\ntwo ways. First, we separate them with a special\\r\\ntoken ([SEP]). Second, we add a learned embed\\x02ding to every token indicating whether it belongs\\r\\nto sentence A or sentence B. As shown in Figure 1,\\r\\nwe denote input embedding as E, the final hidden\\r\\nvector of the special [CLS] token as C ∈ R\\r\\nH,\\r\\nand the final hidden vector for the i\\r\\nth input token\\r\\nas Ti ∈ R\\r\\nH.\\r\\nFor a given token, its input representation is\\r\\nconstructed by summing the corresponding token,\\r\\nsegment, and position embeddings. A visualiza\\x02tion of this construction can be seen in Figure 2.\\r\\n3.1 Pre-training BERT\\r\\nUnlike Peters et al. (2018a) and Radford et al.\\r\\n(2018), we do not use traditional left-to-right or\\r\\nright-to-left language models to pre-train BERT.\\r\\nInstead, we pre-train BERT using two unsuper\\x02vised tasks, described in this section. This step\\r\\nis presented in the left part of Figure 1.'),\n",
              " Document(metadata={'page': 12, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='embedding and the second receives the B embed\\x02ding. 50% of the time B is the actual next sentence\\r\\nthat follows A and 50% of the time it is a random\\r\\nsentence, which is done for the “next sentence pre\\x02diction” task. They are sampled such that the com\\x02bined length is ≤ 512 tokens. The LM masking is\\r\\napplied after WordPiece tokenization with a uni\\x02form masking rate of 15%, and no special consid\\x02eration given to partial word pieces.\\r\\nWe train with batch size of 256 sequences (256\\r\\nsequences * 512 tokens = 128,000 tokens/batch)\\r\\nfor 1,000,000 steps, which is approximately 40\\r\\nepochs over the 3.3 billion word corpus. We\\r\\nuse Adam with learning rate of 1e-4, β1 = 0.9,\\r\\nβ2 = 0.999, L2 weight decay of 0.01, learning\\r\\nrate warmup over the first 10,000 steps, and linear\\r\\ndecay of the learning rate. We use a dropout prob\\x02ability of 0.1 on all layers. We use a gelu acti\\x02vation (Hendrycks and Gimpel, 2016) rather than\\r\\nthe standard relu, following OpenAI GPT. The'),\n",
              " Document(metadata={'page': 4, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='as Parikh et al. (2016); Seo et al. (2017). BERT\\r\\ninstead uses the self-attention mechanism to unify\\r\\nthese two stages, as encoding a concatenated text\\r\\npair with self-attention effectively includes bidi\\x02rectional cross attention between two sentences.\\r\\nFor each task, we simply plug in the task\\x02specific inputs and outputs into BERT and fine\\x02tune all the parameters end-to-end. At the in\\x02put, sentence A and sentence B from pre-training\\r\\nare analogous to (1) sentence pairs in paraphras\\x02ing, (2) hypothesis-premise pairs in entailment, (3)\\r\\nquestion-passage pairs in question answering, and\\r\\n(4) a degenerate text-∅ pair in text classification\\r\\nor sequence tagging. At the output, the token rep\\x02resentations are fed into an output layer for token\\x02level tasks, such as sequence tagging or question\\r\\nanswering, and the [CLS] representation is fed\\r\\ninto an output layer for classification, such as en\\x02tailment or sentiment analysis.'),\n",
              " Document(metadata={'page': 4, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='model pre-training. For the pre-training corpus we\\r\\nuse the BooksCorpus (800M words) (Zhu et al.,\\r\\n2015) and English Wikipedia (2,500M words).\\r\\nFor Wikipedia we extract only the text passages\\r\\nand ignore lists, tables, and headers. It is criti\\x02cal to use a document-level corpus rather than a\\r\\nshuffled sentence-level corpus such as the Billion\\r\\nWord Benchmark (Chelba et al., 2013) in order to\\r\\nextract long contiguous sequences.\\r\\n3.2 Fine-tuning BERT\\r\\nFine-tuning is straightforward since the self\\x02attention mechanism in the Transformer al\\x02lows BERT to model many downstream tasks—\\r\\nwhether they involve single text or text pairs—by\\r\\nswapping out the appropriate inputs and outputs.\\r\\nFor applications involving text pairs, a common\\r\\npattern is to independently encode text pairs be\\x02fore applying bidirectional cross attention, such\\r\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\r\\ninstead uses the self-attention mechanism to unify\\r\\nthese two stages, as encoding a concatenated text')]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What are the training methodologies used by BERT, and how does it incorporate a unified architecture, pre-training, and fine-tuning phases?\"\n",
        "\n",
        "doc_search=retrieve_query(our_query, k=5) # first five most similar texts are returned\n",
        "doc_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAF5f2IaLdFX",
        "outputId": "e5c4343b-36de-4132-abeb-45d2be3e157a"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 2, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='We introduce BERT and its detailed implementa\\x02tion in this section. There are two steps in our\\r\\nframework: pre-training and fine-tuning. Dur\\x02ing pre-training, the model is trained on unlabeled\\r\\ndata over different pre-training tasks. For fine\\x02tuning, the BERT model is first initialized with\\r\\nthe pre-trained parameters, and all of the param\\x02eters are fine-tuned using labeled data from the\\r\\ndownstream tasks. Each downstream task has sep\\x02arate fine-tuned models, even though they are ini\\x02tialized with the same pre-trained parameters. The\\r\\nquestion-answering example in Figure 1 will serve\\r\\nas a running example for this section.\\r\\nA distinctive feature of BERT is its unified ar\\x02chitecture across different tasks. There is mini\\x02mal difference between the pre-trained architec\\x02ture and the final downstream architecture.'),\n",
              " Document(metadata={'page': 2, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='4173\\r\\nBERT BERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nQuestion Paragraph\\r\\nStart/End Span\\r\\nBERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nMasked Sentence A Masked Sentence B\\r\\nPre-training Fine-Tuning\\r\\nNSP Mask LM Mask LM\\r\\nUnlabeled Sentence A and B Pair \\r\\nSQuAD\\r\\nQuestion Answer Pair\\r\\nMNLI NER\\r\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\r\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\r\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\\x02tions/answers).\\r\\ning and auto-encoder objectives have been used'),\n",
              " Document(metadata={'page': 2, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='A distinctive feature of BERT is its unified ar\\x02chitecture across different tasks. There is mini\\x02mal difference between the pre-trained architec\\x02ture and the final downstream architecture.\\r\\nModel Architecture BERT’s model architec\\x02ture is a multi-layer bidirectional Transformer en\\x02coder based on the original implementation de\\x02scribed in Vaswani et al. (2017) and released in\\r\\nthe tensor2tensor library.1 Because the use\\r\\nof Transformers has become common and our im\\x02plementation is almost identical to the original,\\r\\nwe will omit an exhaustive background descrip\\x02tion of the model architecture and refer readers to\\r\\nVaswani et al. (2017) as well as excellent guides\\r\\nsuch as “The Annotated Transformer.”2\\r\\nIn this work, we denote the number of layers\\r\\n(i.e., Transformer blocks) as L, the hidden size as\\r\\nH, and the number of self-attention heads as A.\\r\\n3\\r\\nWe primarily report results on two model sizes:\\r\\nBERTBASE (L=12, H=768, A=12, Total Param\\x02eters=110M) and BERTLARGE (L=24, H=1024,'),\n",
              " Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='to create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan'),\n",
              " Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='example, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked')]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"How does BERT perform on benchmarks like GLUE, SQuAD, and SWAG, and what are the specific improvements achieved by BERTLARGE over other models?\"\n",
        "doc_search=retrieve_query(our_query, k=5) # first five most similar texts are returned\n",
        "doc_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voLK87_CP47M",
        "outputId": "961f5601-a546-49da-e34e-58c3adeb299b"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 5, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='4.5% and 7.0% respective average accuracy im\\x02provement over the prior state of the art. Note that\\r\\nBERTBASE and OpenAI GPT are nearly identical\\r\\nin terms of model architecture apart from the at\\x02tention masking. For the largest and most widely\\r\\nreported GLUE task, MNLI, BERT obtains a 4.6%\\r\\nabsolute accuracy improvement. On the official\\r\\nGLUE leaderboard10, BERTLARGE obtains a score\\r\\nof 80.5, compared to OpenAI GPT, which obtains\\r\\n72.8 as of the date of writing.\\r\\nWe find that BERTLARGE significantly outper\\x02forms BERTBASE across all tasks, especially those\\r\\nwith very little training data. The effect of model\\r\\nsize is explored more thoroughly in Section 5.2.\\r\\n4.2 SQuAD v1.1\\r\\nThe Stanford Question Answering Dataset\\r\\n(SQuAD v1.1) is a collection of 100k crowd\\x02sourced question/answer pairs (Rajpurkar et al.,\\r\\n2016). Given a question and a passage from\\r\\n9The GLUE data set distribution does not include the Test\\r\\nlabels, and we only made a single GLUE evaluation server'),\n",
              " Document(metadata={'page': 5, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\\r\\nWe use a batch size of 32 and fine-tune for 3\\r\\nepochs over the data for all GLUE tasks. For each\\r\\ntask, we selected the best fine-tuning learning rate\\r\\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\\r\\nAdditionally, for BERTLARGE we found that fine\\x02tuning was sometimes unstable on small datasets,\\r\\nso we ran several random restarts and selected the\\r\\nbest model on the Dev set. With random restarts,\\r\\nwe use the same pre-trained checkpoint but per\\x02form different fine-tuning data shuffling and clas\\x02sifier layer initialization.9\\r\\nResults are presented in Table 1. Both\\r\\nBERTBASE and BERTLARGE outperform all sys\\x02tems on all tasks by a substantial margin, obtaining\\r\\n4.5% and 7.0% respective average accuracy im\\x02provement over the prior state of the art. Note that\\r\\nBERTBASE and OpenAI GPT are nearly identical'),\n",
              " Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='to create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan'),\n",
              " Document(metadata={'page': 5, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='4176\\r\\nSystem MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\\r\\n392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\\r\\nPre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0\\r\\nBiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0\\r\\nOpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1\\r\\nBERTBASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\\r\\nBERTLARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\\r\\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).\\r\\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\\r\\nthan the official GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single\\x02model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\\r\\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.'),\n",
              " Document(metadata={'page': 6, 'source': '/content/drive/MyDrive/N19-1423.pdf'}, page_content='A) and a possible continuation (sentence B). The\\r\\nonly task-specific parameters introduced is a vec\\x02tor whose dot product with the [CLS] token rep\\x02resentation C denotes a score for each choice\\r\\nwhich is normalized with a softmax layer.\\r\\nWe fine-tune the model for 3 epochs with a\\r\\nlearning rate of 2e-5 and a batch size of 16. Re\\x02sults are presented in Table 4. BERTLARGE out\\x02performs the authors’ baseline ESIM+ELMo sys\\x02tem by +27.1% and OpenAI GPT by 8.3%.\\r\\n5 Ablation Studies\\r\\nIn this section, we perform ablation experiments\\r\\nover a number of facets of BERT in order to better\\r\\nunderstand their relative importance. Additional')]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G8R4V7BROkz"
      },
      "source": [
        "### Generating an Answer Based on The Similar Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "XNDU0jcma7HB"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template=\"\"\"Use the following pieces of context to answer the user's question of \"{question}\".\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "----------------\n",
        "\"{context}\" \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables =['question','context'],\n",
        "    template = template\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy4fmzsWLayT"
      },
      "source": [
        "### Pipeline For RAG (If you want, you can use the gemini-1.5-pro model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "_aSt7YgIa9jo",
        "outputId": "1dc75535-984d-4198-d772-9a5174a076fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BERT performs exceptionally well on several benchmarks, achieving state-of-the-art results across various natural language processing tasks. Specifically, on the GLUE benchmark, BERTLARGE scores 80.5, which is a 7.7% absolute improvement over the previous state-of-the-art. For the MultiNLI task, BERTLARGE achieves an accuracy of 86.7, marking a 4.6% absolute improvement. In the SQuAD v1.1 benchmark, BERTLARGE reaches a Test F1 score of 93.2, which is a 1.5 point absolute improvement, and for SQuAD v2.0, it scores 83.1, representing a 5.1 point absolute improvement.\\n\\nBERTLARGE significantly outperforms BERTBASE across all tasks, particularly in scenarios with limited training data. The improvements achieved by BERTLARGE over other models, such as OpenAI GPT, include a notable increase in accuracy and F1 scores across the evaluated tasks, demonstrating its effectiveness and robustness in handling various natural language processing challenges.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm=ChatOpenAI(model_name=\"gpt-4o-mini\",\n",
        "               temperature=0,\n",
        "               top_p=1)\n",
        "\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "output= chain.invoke({\"question\":our_query, \"context\":doc_search}) # first five most similar texts are returned\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "2PzfTncDa9dO",
        "outputId": "78d593a9-2b88-4b09-d669-f580d695c69d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "BERT performs exceptionally well on several benchmarks, achieving state-of-the-art results across various natural language processing tasks. Specifically, on the GLUE benchmark, BERTLARGE scores 80.5, which is a 7.7% absolute improvement over the previous state-of-the-art. For the MultiNLI task, BERTLARGE achieves an accuracy of 86.7, marking a 4.6% absolute improvement. In the SQuAD v1.1 benchmark, BERTLARGE reaches a Test F1 score of 93.2, which is a 1.5 point absolute improvement, and for SQuAD v2.0, it scores 83.1, representing a 5.1 point absolute improvement.\n\nBERTLARGE significantly outperforms BERTBASE across all tasks, particularly in scenarios with limited training data. The improvements achieved by BERTLARGE over other models, such as OpenAI GPT, include a notable increase in accuracy and F1 scores across the evaluated tasks, demonstrating its effectiveness and robustness in handling various natural language processing challenges."
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "ioLqLdBrdz4i"
      },
      "outputs": [],
      "source": [
        "def get_answers(query, k=5):\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from IPython.display import Markdown\n",
        "\n",
        "    doc_search=retrieve_query(query, k=k) # most similar texts are returned\n",
        "\n",
        "\n",
        "    template=\"\"\"Use the following pieces of context to answer the user's question of {question}.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    ----------------\n",
        "    {context}\"\"\"\n",
        "\n",
        "    prompt_template = PromptTemplate(\n",
        "    input_variables =['question','context'],\n",
        "    template = template)\n",
        "\n",
        "\n",
        "    llm=ChatOpenAI(model_name=\"gpt-4o-mini\",\n",
        "                  temperature=0,\n",
        "                  top_p=1)\n",
        "\n",
        "    chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "    output= chain.invoke({\"question\":query, \"context\":doc_search}) # first five most similar texts are returned\n",
        "    return Markdown(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "8dEl-8L1eB1j",
        "outputId": "fcfa5d34-8cc9-4bfc-9946-9cac66cca57a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Fine-tuning is a process in machine learning, particularly in natural language processing (NLP), where a pre-trained model is further trained on a specific task with a smaller dataset. This process is relatively inexpensive compared to the initial pre-training phase and typically involves adjusting a few parameters, such as the classification layer weights, to adapt the model to the new task. Fine-tuning allows the model to leverage the knowledge gained during pre-training while specializing in the nuances of the specific task at hand."
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "our_query = \"What is Fine-Tuning?\"\n",
        "answer = get_answers(our_query)\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What is the advantages of Fine-Tuning?\"\n",
        "answer = get_answers(our_query)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "1xlpxtW-NFfr",
        "outputId": "22552df4-2922-4e3f-fd96-8be7cebeecdf"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The advantages of fine-tuning include:\n\n1. **Task-Specific Adaptation**: Fine-tuning allows a pre-trained model to be adapted specifically for a particular task by adding a minimal number of parameters (such as an output layer), which helps in achieving better performance on that task.\n\n2. **Efficiency**: Fine-tuning is relatively inexpensive in terms of computational resources and time. For example, it can be completed in about an hour on a single Cloud TPU or a few hours on a GPU, starting from a pre-trained model.\n\n3. **Utilization of Pre-Trained Knowledge**: Fine-tuning leverages the knowledge gained during pre-training, which can lead to improved performance on downstream tasks compared to training a model from scratch.\n\n4. **Flexibility**: It allows for the exploration of various tasks without the need to retrain the entire model, as only the task-specific parameters need to be learned.\n\n5. **Improved Performance**: Fine-tuning can lead to significant improvements in performance metrics, as seen in various benchmarks like GLUE, where fine-tuned models achieve high accuracy on diverse natural language understanding tasks."
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Kls97b1xzWPI",
        "outputId": "dce3440b-b1d7-4161-f7c3-839786150d2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "BERT handles both single and paired sentences by representing them in a single token sequence. A \"sequence\" can consist of either a single sentence or two sentences packed together. To differentiate between the two sentences in a pair, BERT uses a special token ([SEP]) to separate them and adds a learned embedding to each token indicating whether it belongs to sentence A or sentence B.\n\nBERT utilizes WordPiece embeddings with a vocabulary size of 30,000 tokens. The first token of every sequence is a special classification token ([CLS]), and the final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. The input representation for each token is constructed by summing the corresponding token, segment, and position embeddings."
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "our_query = \"How does BERT handle single and paired sentences, and what kind of embeddings and vocabulary size does it use?\"\n",
        "answer = get_answers(our_query)\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What are the training methodologies used by BERT, and how does it incorporate a unified architecture, pre-training, and fine-tuning phases?\"\n",
        "answer = get_answers(our_query)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "Hzo5lh7KMoza",
        "outputId": "168dbe77-bfe8-433a-e708-8c9511063387"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "BERT employs a two-step training methodology consisting of pre-training and fine-tuning phases. \n\n1. **Pre-training**: During this phase, BERT is trained on unlabeled data using different pre-training tasks, specifically the Masked Language Model (MLM) and Next Sentence Prediction (NSP). The MLM randomly masks some tokens in the input, and the model's objective is to predict the original tokens. NSP involves predicting whether a given sentence follows another sentence, which helps the model understand the relationship between sentences.\n\n2. **Fine-tuning**: After pre-training, the BERT model is initialized with the pre-trained parameters and then fine-tuned on labeled data for specific downstream tasks, such as question answering or named entity recognition. Each downstream task has its own fine-tuned model, but all models are initialized with the same pre-trained parameters. During fine-tuning, all parameters of the model are adjusted based on the labeled data.\n\nA distinctive feature of BERT is its **unified architecture**. There is minimal difference between the architecture used during pre-training and the architecture used during fine-tuning, allowing for a seamless transition between the two phases. This unified approach enables BERT to achieve state-of-the-art results across various natural language processing tasks without requiring substantial modifications to the architecture for each specific task."
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"How does BERT perform on benchmarks like GLUE, SQuAD, and SWAG, and what are the specific improvements achieved by BERTLARGE over other models?\"\n",
        "answer = get_answers(our_query)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "krLNBXGZPrZU",
        "outputId": "b3c4076f-5233-41b8-affb-d61eaf90f749"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "BERT performs exceptionally well on various benchmarks, achieving state-of-the-art results across multiple natural language processing tasks. Specifically, on the GLUE benchmark, BERTLARGE scores 80.5, which is a 7.7% absolute improvement over the previous state of the art. For the MultiNLI task, BERTLARGE achieves an accuracy of 86.7, marking a 4.6% absolute improvement. In the SQuAD v1.1 benchmark, BERTLARGE reaches a Test F1 score of 93.2, which is a 1.5 point absolute improvement, and for SQuAD v2.0, it scores 83.1, representing a 5.1 point absolute improvement.\n\nBERTLARGE significantly outperforms BERTBASE across all tasks, particularly in scenarios with limited training data. The improvements achieved by BERTLARGE over other models, such as OpenAI GPT, are notable, with BERTLARGE outperforming it by 8.3% on the SWAG benchmark and by substantial margins on other tasks as well. Overall, BERT's architecture and training methodology contribute to its superior performance on these benchmarks."
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the test questions and answers\n",
        "test_questions_and_answers = [\n",
        "    {\"question\": \"What is BERT?\",\n",
        "     \"answer\": \"BERT stands for Bidirectional Encoder Representations from Transformers. It is a language representation model that pre-trains deep bidirectional representations by jointly conditioning on both left and right context in all layers.\"},\n",
        "    {\"question\": \"What is the main contribution of the BERT paper?\",\n",
        "     \"answer\": \"The main contribution is demonstrating the importance of bidirectional pre-training for language representations and showing that BERT achieves state-of-the-art performance on various NLP tasks.\"},\n",
        "    {\"question\": \"How does BERT differ from previous models like OpenAI GPT?\",\n",
        "     \"answer\": \"Unlike OpenAI GPT, which uses a unidirectional language model, BERT uses a masked language model (MLM) objective that enables bidirectional representations, and it also uses a 'next sentence prediction' task for text-pair pre-training.\"},\n",
        "    {\"question\": \"What are the two main pre-training tasks used in BERT?\",\n",
        "     \"answer\": \"The two main pre-training tasks are Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\"},\n",
        "    {\"question\": \"What datasets were used to pre-train BERT?\",\n",
        "     \"answer\": \"BERT was pre-trained using the BooksCorpus (800M words) and English Wikipedia (2,500M words).\"},\n",
        "    # Add more question-answer pairs here...\n",
        "]"
      ],
      "metadata": {
        "id": "MJ22amQpKds4"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_query(question):\n",
        "    # Call the get_answers function and convert the Markdown output to plain text\n",
        "    response_md = get_answers(question)\n",
        "    # Extract plain text from the Markdown output\n",
        "    response_text = response_md.data\n",
        "    return response_text\n",
        "\n",
        "def evaluate_chatbot(chatbot_function, test_data):\n",
        "    correct_count = 0\n",
        "    for test in test_data:\n",
        "        response = chatbot_function(test[\"question\"])\n",
        "        print(f\"Question: {test['question']}\")\n",
        "        print(f\"Expected Answer: {test['answer']}\")\n",
        "        print(f\"Chatbot Response: {response}\\n\")\n",
        "        if test[\"answer\"].lower() in response.lower():\n",
        "            correct_count += 1\n",
        "\n",
        "    accuracy = (correct_count / len(test_data)) * 100\n",
        "    return accuracy\n",
        "\n",
        "# Run the evaluation\n",
        "accuracy = evaluate_chatbot(chatbot_query, test_questions_and_answers)\n",
        "# print(f\"Chatbot Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfbMX2M2KSgY",
        "outputId": "8e39fbf0-e173-4a15-e5ce-31716b59b420"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is BERT?\n",
            "Expected Answer: BERT stands for Bidirectional Encoder Representations from Transformers. It is a language representation model that pre-trains deep bidirectional representations by jointly conditioning on both left and right context in all layers.\n",
            "Chatbot Response: BERT, which stands for Bidirectional Encoder Representations from Transformers, is a language representation model introduced to pre-train deep bidirectional representations from unlabeled text. It jointly conditions on both left and right context in all layers, allowing it to create state-of-the-art models for various natural language processing tasks, such as question answering and language inference. BERT uses a masked language model pre-training objective, which involves randomly masking some tokens from the input and predicting the original vocabulary id of the masked tokens. This approach alleviates the unidirectionality constraint found in previous models, enabling better performance on sentence-level and token-level tasks.\n",
            "\n",
            "Question: What is the main contribution of the BERT paper?\n",
            "Expected Answer: The main contribution is demonstrating the importance of bidirectional pre-training for language representations and showing that BERT achieves state-of-the-art performance on various NLP tasks.\n",
            "Chatbot Response: The main contribution of the BERT paper is the introduction of a new language representation model called BERT (Bidirectional Encoder Representations from Transformers), which pre-trains deep bidirectional representations from unlabeled text. This model allows for joint conditioning on both left and right context in all layers, enabling it to achieve state-of-the-art results on a wide range of natural language processing tasks without substantial task-specific architecture modifications. BERT demonstrates the importance of bidirectional pre-training and reduces the need for heavily-engineered task-specific architectures, outperforming many existing models on eleven NLP tasks.\n",
            "\n",
            "Question: How does BERT differ from previous models like OpenAI GPT?\n",
            "Expected Answer: Unlike OpenAI GPT, which uses a unidirectional language model, BERT uses a masked language model (MLM) objective that enables bidirectional representations, and it also uses a 'next sentence prediction' task for text-pair pre-training.\n",
            "Chatbot Response: BERT differs from OpenAI GPT in several key ways:\n",
            "\n",
            "1. **Architecture**: BERT uses a bidirectional Transformer architecture, allowing it to consider context from both the left and right of a token simultaneously. In contrast, GPT employs a left-to-right Transformer architecture, where each token can only attend to previous tokens.\n",
            "\n",
            "2. **Pre-training Objectives**: BERT utilizes a masked language model (MLM) pre-training objective, which randomly masks some tokens in the input and trains the model to predict them. This approach is inspired by the Cloze task. GPT, on the other hand, is trained to predict the next token in a sequence, which is a unidirectional approach.\n",
            "\n",
            "3. **Training Data**: BERT is trained on a larger dataset that includes both the BooksCorpus and Wikipedia, while GPT is trained solely on the BooksCorpus.\n",
            "\n",
            "4. **Token Handling**: In BERT, special tokens like [SEP] and [CLS] are learned during pre-training, whereas in GPT, these tokens are introduced only during fine-tuning.\n",
            "\n",
            "5. **Fine-tuning Approach**: Both BERT and GPT are fine-tuning approaches, but BERT's bidirectional context allows it to perform better on tasks that require understanding of the entire sentence, such as question answering.\n",
            "\n",
            "These differences contribute to BERT's ability to achieve state-of-the-art results on various natural language processing tasks.\n",
            "\n",
            "Question: What are the two main pre-training tasks used in BERT?\n",
            "Expected Answer: The two main pre-training tasks are Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\n",
            "Chatbot Response: The two main pre-training tasks used in BERT are:\n",
            "\n",
            "1. Masked Language Model (MLM)\n",
            "2. Next Sentence Prediction (NSP)\n",
            "\n",
            "Question: What datasets were used to pre-train BERT?\n",
            "Expected Answer: BERT was pre-trained using the BooksCorpus (800M words) and English Wikipedia (2,500M words).\n",
            "Chatbot Response: BERT was pre-trained using two datasets: BooksCorpus, which contains 800 million words, and English Wikipedia, which contains 2,500 million words.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7rM6NaRFoK-",
        "outputId": "14111e3c-de81-40a9-b6b8-af8916018c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.42.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.112.2)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.5)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.38.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.42.0-py3-none-any.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, ffmpy, aiofiles, gradio-client, gradio\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 13.0.1\n",
            "    Uninstalling websockets-13.0.1:\n",
            "      Successfully uninstalled websockets-13.0.1\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.2\n",
            "    Uninstalling tomlkit-0.13.2:\n",
            "      Successfully uninstalled tomlkit-0.13.2\n",
            "Successfully installed aiofiles-23.2.1 ffmpy-0.4.0 gradio-4.42.0 gradio-client-1.3.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.6.3 semantic-version-2.10.0 tomlkit-0.12.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "_o4zJ1n_qIjc",
        "outputId": "568292b5-0866-4a19-a3d4-cd3a13a39f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://7a77ebe233a1291006.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7a77ebe233a1291006.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# we create a simple web interface using the Gradio library.\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def get_answer(query):\n",
        "    return get_answers(query).data  # Use .data to get the string content from Markdown\n",
        "\n",
        "interface = gr.Interface(fn=get_answer, inputs=\"text\", outputs=\"markdown\")\n",
        "interface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9GmKlL2NRff"
      },
      "source": [
        "## Project 2: Generating PDF Document Summaries\n",
        "\n",
        "In this project, you will explore various methods for creating summaries from the provided PDF document. You will experiment with different chaining functions offered by the Langchain library to achieve this.\n",
        "\n",
        "### **Project Steps:**\n",
        "- **1.PDF Document Upload and Chunking:** As in the first project, upload the PDF document and divide it into smaller chunks. Consider splitting it by half-page or page.\n",
        "\n",
        "- **2.Summarization Techniques:**\n",
        "\n",
        "  - **Summary of the First 5 Pages (Stuff Chain):** Utilize the load_summarize_chain function with the parameter chain_type=\"stuff\" to generate a concise summary of the first 5 pages of the PDF document.\n",
        "\n",
        "  - **Short Summary of the Entire Document (Map Reduce Chain):** Employ chain_type=\"map_reduce\" and refine parameters to create a brief summary of the entire document. This method generates individual summaries for each chunk and then combines them into a final summary.\n",
        "\n",
        "  - **Detailed Summary with Bullet Points (Map Reduce Chain):** Use chain_type=\"map_reduce\" to generate a detailed summary with at least 1000 tokens. Provide the LLM with the prompt \"Summarize with 1000 tokens\" and set the max_token parameter to a value greater than 1000. Add a title to the summary and present key points using bullet points.\n",
        "\n",
        "### Important Notes:\n",
        "\n",
        "- Models like GPT-4 and Gemini Pro models might excel in generating summaries based on token count. Consider prioritizing these models.\n",
        "\n",
        "- For comprehensive information on Langchain and LLMs, refer to their respective documentation.\n",
        "Best of luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhjLe0IqRnl4"
      },
      "source": [
        "### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZXdV8CcqbFrW"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "yJ4Mh3VJYL3G"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rcFsXQwCbFkm"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2l3uLT1EYUrd"
      },
      "outputs": [],
      "source": [
        "!pip install -qU pypdfium2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqImlx_IRqQS"
      },
      "source": [
        "### Loading PDF Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "CCkT3msfbH_n"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFium2Loader\n",
        "\n",
        "def read_doc(directory):\n",
        "    file_loader=PyPDFium2Loader(directory)\n",
        "    pdf_documents=file_loader.load()\n",
        "    return pdf_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a_FpBOcbHzP",
        "outputId": "44ffff51-f93a-4c36-9992-11118537ec6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "pdf=read_doc('/content/drive/MyDrive/N19-1423.pdf')\n",
        "len(pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuyT0IoWR4n8"
      },
      "source": [
        "### Summarizing the First 5 Pages of The Document With Chain_Type of The 'stuff'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "O3yAnW3PbKIX"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain   # load_summarize_chain is a chain that automates the summarization process offered by langchain\n",
        "\n",
        "llm = ChatOpenAI(temperature=0,                               # we define the model\n",
        "                 model_name='gpt-4o-mini',\n",
        "                 max_tokens=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "8wopgGPibKA3"
      },
      "outputs": [],
      "source": [
        "chain = load_summarize_chain(                                 # we create a summarization chain that we will use to summarize texts\n",
        "    llm,\n",
        "    chain_type='stuff'\n",
        ")\n",
        "output_summary = chain.invoke(pdf[0:5])['output_text']        # summarizes the given text using the summarization chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "5j9NMbSCbMyf",
        "outputId": "438e36da-fc43-42dd-8993-f87758be3201"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. BERT pre-trains deep bidirectional representations from unlabeled text by jointly considering both left and right contexts, overcoming limitations of previous unidirectional models. It employs a masked language model (MLM) and a next sentence prediction (NSP) task during pre-training, allowing it to achieve state-of-the-art results on eleven natural language processing tasks, including question answering and language inference. BERT's architecture is simple and effective, requiring minimal task-specific modifications during fine-tuning, and it significantly improves performance benchmarks such as GLUE and SQuAD. The model's code and pre-trained versions are publicly available for further research and application."
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "Markdown(output_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53mpwb7KbMrf",
        "outputId": "55586e90-65c1-4460-a337-483113b04038"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d10a17f3e50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d10a05659c0>, root_client=<openai.OpenAI object at 0x7d10a17fc4c0>, root_async_client=<openai.AsyncOpenAI object at 0x7d10a17f3d30>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OHFTJkl8b_LM",
        "outputId": "514f3edf-b5f9-4662-a2eb-719d2bb77b8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "chain.llm_chain.prompt.template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ix4V4w7FcCUn"
      },
      "outputs": [],
      "source": [
        "chain.llm_chain.prompt.template=\"\"\"Write a summary in 1000 tokens of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:\"\"\"\n",
        "\n",
        "# If we think the summary is short, we can change the template for a more detailed summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNOTWOcacNkG",
        "outputId": "588fa8e9-2a64-401d-d294-60d702cc272c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a summary in 1000 tokens of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d10a17f3e50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d10a05659c0>, root_client=<openai.OpenAI object at 0x7d10a17fc4c0>, root_async_client=<openai.AsyncOpenAI object at 0x7d10a17f3d30>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "jejXdEF2cSL2",
        "outputId": "c8513d45-aa0c-46a3-c2fc-28536d2d801f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. BERT distinguishes itself from previous models by employing a deep bidirectional approach, allowing it to consider both left and right context during pre-training. This is achieved through a \"masked language model\" (MLM) objective, where random tokens in the input are masked, and the model learns to predict these masked tokens based on their surrounding context. Additionally, BERT incorporates a \"next sentence prediction\" (NSP) task to enhance its understanding of sentence relationships, which is crucial for tasks like question answering and natural language inference.\\n\\nBERT\\'s architecture is based on the Transformer model, consisting of multiple layers of bidirectional encoders. It is pre-trained on large corpora, including BooksCorpus and English Wikipedia, and can be fine-tuned for various downstream tasks with minimal task-specific modifications. The model\\'s unified architecture allows it to handle both single and paired sentence inputs effectively.\\n\\nThe results demonstrate BERT\\'s effectiveness, achieving state-of-the-art performance on eleven natural language processing tasks, including significant improvements in GLUE scores, MultiNLI accuracy, and SQuAD question answering benchmarks. The paper emphasizes the importance of bidirectional pre-training and shows that BERT reduces the need for complex task-specific architectures, making it a powerful tool for a wide range of NLP applications. The code and pre-trained models are made publicly available for further research and application.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model_name='gpt-4o-mini',\n",
        "                 max_tokens=1024)          # max_tokens must be greater than 1000; otherwise, the prompt we have given will be meaningless.\n",
        "\n",
        "chain = load_summarize_chain(\n",
        "    llm,\n",
        "    chain_type='stuff',\n",
        "    #prompt=prompt\n",
        ")\n",
        "output_summary = chain.invoke(pdf[0:5])['output_text']\n",
        "output_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YosGDw4w7bh4"
      },
      "outputs": [],
      "source": [
        "# returned an output corresponding to 1000 tokens depending on the tokenization done by the model in the background"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "ZUfCt_Y6cWJF",
        "outputId": "f3f1aa7e-447a-466e-ffe0-b7f477599a82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. BERT distinguishes itself from previous models by employing a deep bidirectional approach, allowing it to consider both left and right context during pre-training. This is achieved through a \"masked language model\" (MLM) objective, where random tokens in the input are masked, and the model learns to predict these masked tokens based on their surrounding context. Additionally, BERT incorporates a \"next sentence prediction\" (NSP) task to enhance its understanding of sentence relationships, which is crucial for tasks like question answering and natural language inference.\n\nBERT's architecture is based on the Transformer model, consisting of multiple layers of bidirectional encoders. It is pre-trained on large corpora, including BooksCorpus and English Wikipedia, and can be fine-tuned for various downstream tasks with minimal task-specific modifications. The model's unified architecture allows it to handle both single and paired sentence inputs effectively.\n\nThe results demonstrate BERT's effectiveness, achieving state-of-the-art performance on eleven natural language processing tasks, including significant improvements in GLUE scores, MultiNLI accuracy, and SQuAD question answering benchmarks. The paper emphasizes the importance of bidirectional pre-training and shows that BERT reduces the need for complex task-specific architectures, making it a powerful tool for a wide range of NLP applications. The code and pre-trained models are made publicly available for further research and application."
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "Markdown(output_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Z72HUBMkcbDx"
      },
      "outputs": [],
      "source": [
        "chain.llm_chain.prompt.template=\"\"\"Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvrLsoivTulb"
      },
      "source": [
        "### Document Splitter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2024-09-01 151402.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "_rtci718zy0d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "t6wPW3OFbOcJ"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import textwrap\n",
        "\n",
        "llm = ChatOpenAI(temperature=0,                               # we define the model\n",
        "                 model_name='gpt-4o-mini',\n",
        "                 max_tokens=1024)\n",
        "\n",
        "# map reduce is used for longer texts\n",
        "# For example, if we're using gpt-4o-mini, the maximum input capacity is 128000 tokens.\n",
        "# Assuming 1000 tokens roughly correspond to 1 page, 128000 tokens would be about 128 pages.\n",
        "# We can provide a text of up to 128 pages to the GPT model in one go.\n",
        "# However, for a text of 5000 pages, we would need to use methods like map_reduce or refine to summarize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "8NpJYGxRbOVC"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
        "chunks = text_splitter.split_documents(pdf)\n",
        "\n",
        "# In RAG chatbots, we can set the chunk size to correspond to a quarter or a fifth of a page.\n",
        "# However, it is recommended to divide chunks so that each is approximately one page when summarizing.\n",
        "# For text in Arial 11-point font, 10000 characters roughly equal one page.\n",
        "\n",
        "# There's no need to use chunk_overlap because maintaining topic continuity isn't necessary here; we will be using the entire text anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKtJFWMgepMX",
        "outputId": "f3b0f863-03b0-43ce-f579-0567e3b9fb09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK-yznsneyJz",
        "outputId": "d6eabee4-98cc-4624-c365-65dfa3c694ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/drive/MyDrive/N19-1423.pdf', 'page': 0}, page_content='Proceedings of NAACL-HLT 2019, pages 4171–4186\\r\\nMinneapolis, Minnesota, June 2 - June 7, 2019. \\rc 2019 Association for Computational Linguistics\\r\\n4171\\r\\nBERT: Pre-training of Deep Bidirectional Transformers for\\r\\nLanguage Understanding\\r\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\r\\nGoogle AI Language\\r\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\r\\nAbstract\\r\\nWe introduce a new language representa\\x02tion model called BERT, which stands for\\r\\nBidirectional Encoder Representations from\\r\\nTransformers. Unlike recent language repre\\x02sentation models (Peters et al., 2018a; Rad\\x02ford et al., 2018), BERT is designed to pre\\x02train deep bidirectional representations from\\r\\nunlabeled text by jointly conditioning on both\\r\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\r\\nto create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan\\r\\nand Brockett, 2005), which aim to predict the re\\x02lationships between sentences by analyzing them\\r\\nholistically, as well as token-level tasks such as\\r\\nnamed entity recognition and question answering,\\r\\nwhere models are required to produce fine-grained\\r\\noutput at the token level (Tjong Kim Sang and\\r\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\r\\nThere are two existing strategies for apply\\x02ing pre-trained language representations to down\\x02stream tasks: feature-based and fine-tuning. The\\r\\nfeature-based approach, such as ELMo (Peters\\r\\net al., 2018a), uses task-specific architectures that\\r\\ninclude the pre-trained representations as addi\\x02tional features. The fine-tuning approach, such as\\r\\nthe Generative Pre-trained Transformer (OpenAI\\r\\nGPT) (Radford et al., 2018), introduces minimal\\r\\ntask-specific parameters, and is trained on the\\r\\ndownstream tasks by simply fine-tuning all pre\\x02trained parameters. The two approaches share the\\r\\nsame objective function during pre-training, where\\r\\nthey use unidirectional language models to learn\\r\\ngeneral language representations.\\r\\nWe argue that current techniques restrict the\\r\\npower of the pre-trained representations, espe\\x02cially for the fine-tuning approaches. The ma\\x02jor limitation is that standard language models are\\r\\nunidirectional, and this limits the choice of archi\\x02tectures that can be used during pre-training. For\\r\\nexample, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "chunks[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zlVe2iISX0Q"
      },
      "source": [
        "### Make A Brief Summary of The Entire Document With Chain_Types of \"map_reduce\" and \"refine\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Ta9kxveDe03n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "c95d55a9-7bb1-41d1-9c06-67a31b9110aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.95 s, sys: 403 ms, total: 3.36 s\n",
            "Wall time: 4min 53s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper presents BERT (Bidirectional Encoder Representations from Transformers), a groundbreaking language representation model developed by Google AI Language. BERT utilizes deep bidirectional representations, allowing it to consider both left and right context during pre-training on unlabeled text, which enhances its performance on various natural language processing (NLP) tasks. It employs a masked language model (MLM) and next sentence prediction (NSP) for pre-training, enabling effective fine-tuning with minimal task-specific adjustments. BERT achieves state-of-the-art results on eleven NLP benchmarks, including GLUE and SQuAD, outperforming previous models like OpenAI GPT and ELMo. The model's architecture includes a multi-layer bidirectional Transformer encoder, and it supports versatile input representations for single and paired sentences. The study highlights the significance of pre-training duration, masking strategies, and model size on performance, demonstrating that BERT's design and training methodology lead to substantial improvements in various NLP tasks. The code and pre-trained models are publicly available for further research and application."
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "%%time\n",
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"map_reduce\")    # loads a summarization chain using a language model thet we defined earlier .\n",
        "                                                         # and specifies the chain type as \"map-reduce\"\n",
        "\n",
        "output_summary = chain.invoke(chunks)[\"output_text\"]     # It generates a summary for each of the 16 pages individually,\n",
        "Markdown(output_summary)                                 # then combines the 16 summaries to produce a final summary. In total, this process is completed in 17 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWXW7dlPe5MT",
        "outputId": "ffb1af77-5ad7-4853-f3ea-95e7e91fde31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d10a0882bf0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d10a0881510>, root_client=<openai.OpenAI object at 0x7d10a08736a0>, root_async_client=<openai.AsyncOpenAI object at 0x7d10a0882c20>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d10a0882bf0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d10a0881510>, root_client=<openai.OpenAI object at 0x7d10a08736a0>, root_async_client=<openai.AsyncOpenAI object at 0x7d10a0882c20>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text')), document_variable_name='text')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dDnLpVCYe736",
        "outputId": "6a7017c8-e6e4-4a56-99cf-cfa9989a8319"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# for summarizing each part/chunk\n",
        "chain.llm_chain.prompt.template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "16sBpgLge-0o",
        "outputId": "2ef064f5-2b97-4032-ba55-9da69f2fe905"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# for combining the parts\n",
        "chain.combine_document_chain.llm_chain.prompt.template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNYQLBRLc0kf"
      },
      "outputs": [],
      "source": [
        "# If there is more than one template in the chain, we cannot edit the prompts of these templates separately.\n",
        "# When we change any template within the chain, all templates in the chain will change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zZxse-ZUV3S"
      },
      "source": [
        "### Generate A Detailed Summary of The Entire Document With At Least 1000 Tokens. Also, Add A Title To The Summary And Present Key Points Using Bullet Points With Chain_Type of \"map_reduce\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpuiEedJbQME",
        "outputId": "bf09fa83-64a0-4978-ab4b-ea2b9cea7445"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d10a0882bf0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d10a0881510>, root_client=<openai.OpenAI object at 0x7d10a08736a0>, root_async_client=<openai.AsyncOpenAI object at 0x7d10a0882c20>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d10a0882bf0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d10a0881510>, root_client=<openai.OpenAI object at 0x7d10a08736a0>, root_async_client=<openai.AsyncOpenAI object at 0x7d10a0882c20>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text')), document_variable_name='text')"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type='map_reduce'\n",
        ")\n",
        "chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "OdFF9bOAbQEo"
      },
      "outputs": [],
      "source": [
        "# prompt for every chunk\n",
        "from langchain import PromptTemplate                         # we edit the prompt of the large language model that summarizes each chunk\n",
        "\n",
        "chunks_prompt=\"\"\"\n",
        "Please summarize the below text:\n",
        "text:'{text}'\n",
        "summary:\n",
        "\"\"\"\n",
        "map_prompt_template=PromptTemplate(input_variables=['text'],\n",
        "                                   template=chunks_prompt)\n",
        "\n",
        "# text:'{text}'\n",
        "# summary:\n",
        "# this part allows the model to better understand our instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "qF-yZPWkyghe"
      },
      "outputs": [],
      "source": [
        "# prompt for combined summaries\n",
        "final_combine_prompt=\"\"\"\n",
        "Provide a final summary of the entire text with at least 1000 tokens with important points.\n",
        "Add a generic title,\n",
        "Start the precise summary with an introduction and provide the summary in bullet points.\n",
        "text: '{text}'\n",
        "summary:\n",
        "\"\"\"\n",
        "final_combine_prompt_template=PromptTemplate(input_variables=['text'],\n",
        "                                             template=final_combine_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlcFRehlyjMg",
        "outputId": "820432ef-3505-42f4-8028-bf5808665b75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template=\"\\nPlease summarize the below text:\\ntext:'{text}'\\nsummary:\\n\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d10a0882bf0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d10a0881510>, root_client=<openai.OpenAI object at 0x7d10a08736a0>, root_async_client=<openai.AsyncOpenAI object at 0x7d10a0882c20>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template=\"\\nProvide a final summary of the entire text with at least 1000 tokens with important points.\\nAdd a generic title,\\nStart the precise summary with an introduction and provide the summary in bullet points.\\ntext: '{text}'\\nsummary:\\n\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d10a0882bf0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d10a0881510>, root_client=<openai.OpenAI object at 0x7d10a08736a0>, root_async_client=<openai.AsyncOpenAI object at 0x7d10a0882c20>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text')), document_variable_name='text')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "chain = load_summarize_chain(\n",
        "                            llm=llm,\n",
        "                            chain_type='map_reduce',\n",
        "                            map_prompt=map_prompt_template,               # prompt to summarize each chunk\n",
        "                            combine_prompt=final_combine_prompt_template  # the prompt that will put all the summaries together and produce the final summary\n",
        ")\n",
        "chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "jP5Zs4tByvbm",
        "outputId": "6be888a0-c85e-45e8-dd3a-a51b5728360a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Comprehensive Overview of BERT: Innovations in Language Representation Models\\n\\n## Introduction\\nBERT (Bidirectional Encoder Representations from Transformers) is a revolutionary language representation model developed by Google AI Language that has transformed the landscape of natural language processing (NLP). By utilizing deep bidirectional representations from unlabeled text, BERT is capable of understanding context from both directions—left and right—leading to significant improvements in performance across various NLP tasks. This summary encapsulates the key contributions, methodologies, and findings related to BERT, highlighting its impact on the field of NLP.\\n\\n## Summary of Key Points\\n\\n### BERT's Core Innovations\\n- **Bidirectional Pre-training**: \\n  - BERT employs a masked language model (MLM) that allows it to learn from both left and right contexts, contrasting with previous unidirectional models that only considered one direction.\\n  \\n- **Reduction of Task-Specific Architectures**: \\n  - The pre-trained representations of BERT reduce the necessity for complex, task-specific models, enabling it to achieve state-of-the-art performance across a variety of NLP tasks.\\n\\n- **Performance Improvement**: \\n  - BERT has outperformed existing models on eleven NLP tasks, with its code and pre-trained models made publicly available for further research and application.\\n\\n### Training Methodologies\\n- **Unified Architecture**: \\n  - BERT utilizes a multi-layer bidirectional Transformer encoder, which is effective for a wide range of NLP tasks.\\n\\n- **Pre-training Phase**: \\n  - The model is trained on unlabeled data using two primary tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\\n  \\n- **Fine-tuning Phase**: \\n  - The pre-trained model is adapted to specific downstream tasks using labeled data, with minimal new parameters introduced primarily for classification layers.\\n\\n### Input and Output Representations\\n- **Tokenization**: \\n  - BERT employs WordPiece embeddings with a vocabulary of 30,000 tokens, effectively handling both single and paired sentences.\\n\\n- **Special Tokens**: \\n  - Each input sequence begins with a classification token ([CLS]), and sentences are separated using a special token ([SEP]).\\n\\n### Pre-training Tasks\\n1. **Masked Language Model (MLM)**: \\n   - Randomly masks a percentage of input tokens and predicts them, facilitating deep bidirectional representation.\\n   \\n2. **Next Sentence Prediction (NSP)**: \\n   - Trains the model to understand relationships between sentences by predicting whether a second sentence logically follows the first.\\n\\n### Performance on Benchmarks\\n- **GLUE Benchmark**: \\n  - BERT models significantly outperform all others across various tasks, with BERTLARGE achieving an average accuracy improvement of 7.0% over the previous best.\\n\\n- **SQuAD v1.1 and v2.0**: \\n  - BERTLARGE achieves an F1 score of 87.4 on SQuAD 1.1, surpassing other models, including human performance, and shows notable improvement on SQuAD 2.0.\\n\\n- **SWAG Dataset**: \\n  - BERTLARGE excels in grounded common-sense inference tasks, outperforming other models by substantial margins.\\n\\n### Insights from Ablation Studies\\n- **Impact of Pre-training Tasks**: \\n  - Omitting the NSP task leads to a significant drop in performance, underscoring the importance of bidirectionality in training.\\n\\n- **Model Size**: \\n  - Larger BERT models consistently achieve better accuracy, indicating that increased model size enhances performance.\\n\\n- **Importance of Extensive Pre-training**: \\n  - BERT requires extensive pre-training (1 million steps) to achieve optimal performance, with a notable increase in accuracy observed when pre-training is extended.\\n\\n- **Comparison of Masking Strategies**: \\n  - The MLM approach converges at a slower rate compared to left-to-right models but ultimately outperforms them in accuracy, highlighting its effectiveness in capturing contextual relationships.\\n\\n- **Challenges with Masking Strategies**: \\n  - The study identifies limitations with the MASK strategy in feature-based approaches, particularly in named entity recognition (NER) tasks.\\n\\n- **Effectiveness of Mixed Masking Strategy**: \\n  - BERT's mixed masking strategy enhances model performance, while reliance on random tokens for masking leads to suboptimal results.\\n\\n### Fine-tuning Approaches\\n- **Feature-based vs. Fine-tuning**: \\n  - The feature-based approach offers computational advantages and flexibility, performing competitively with state-of-the-art methods.\\n\\n### Hyperparameter Tuning\\n- **Sensitivity to Hyperparameters**: \\n  - Larger datasets exhibit less sensitivity to hyperparameter choices, indicating the necessity for exhaustive searches for optimal parameters.\\n\\n### Comparison with Other Models\\n- **Architectural Differences**: \\n  - BERT's bidirectional architecture and pre-training tasks significantly contribute to its performance improvements compared to models like OpenAI GPT and ELMo.\\n\\n### Conclusion\\nBERT's architecture and training strategies signify a substantial advancement in natural language understanding\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "output_summary = chain.invoke(chunks)[\"output_text\"]\n",
        "output_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "4145m9mcyyKA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e816dd32-6366-47d5-db8c-a196915e7611"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Comprehensive Overview of BERT: Innovations in Language Representation Models\n\n## Introduction\nBERT (Bidirectional Encoder Representations from Transformers) is a revolutionary language representation model developed by Google AI Language that has transformed the landscape of natural language processing (NLP). By utilizing deep bidirectional representations from unlabeled text, BERT is capable of understanding context from both directions—left and right—leading to significant improvements in performance across various NLP tasks. This summary encapsulates the key contributions, methodologies, and findings related to BERT, highlighting its impact on the field of NLP.\n\n## Summary of Key Points\n\n### BERT's Core Innovations\n- **Bidirectional Pre-training**: \n  - BERT employs a masked language model (MLM) that allows it to learn from both left and right contexts, contrasting with previous unidirectional models that only considered one direction.\n  \n- **Reduction of Task-Specific Architectures**: \n  - The pre-trained representations of BERT reduce the necessity for complex, task-specific models, enabling it to achieve state-of-the-art performance across a variety of NLP tasks.\n\n- **Performance Improvement**: \n  - BERT has outperformed existing models on eleven NLP tasks, with its code and pre-trained models made publicly available for further research and application.\n\n### Training Methodologies\n- **Unified Architecture**: \n  - BERT utilizes a multi-layer bidirectional Transformer encoder, which is effective for a wide range of NLP tasks.\n\n- **Pre-training Phase**: \n  - The model is trained on unlabeled data using two primary tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\n  \n- **Fine-tuning Phase**: \n  - The pre-trained model is adapted to specific downstream tasks using labeled data, with minimal new parameters introduced primarily for classification layers.\n\n### Input and Output Representations\n- **Tokenization**: \n  - BERT employs WordPiece embeddings with a vocabulary of 30,000 tokens, effectively handling both single and paired sentences.\n\n- **Special Tokens**: \n  - Each input sequence begins with a classification token ([CLS]), and sentences are separated using a special token ([SEP]).\n\n### Pre-training Tasks\n1. **Masked Language Model (MLM)**: \n   - Randomly masks a percentage of input tokens and predicts them, facilitating deep bidirectional representation.\n   \n2. **Next Sentence Prediction (NSP)**: \n   - Trains the model to understand relationships between sentences by predicting whether a second sentence logically follows the first.\n\n### Performance on Benchmarks\n- **GLUE Benchmark**: \n  - BERT models significantly outperform all others across various tasks, with BERTLARGE achieving an average accuracy improvement of 7.0% over the previous best.\n\n- **SQuAD v1.1 and v2.0**: \n  - BERTLARGE achieves an F1 score of 87.4 on SQuAD 1.1, surpassing other models, including human performance, and shows notable improvement on SQuAD 2.0.\n\n- **SWAG Dataset**: \n  - BERTLARGE excels in grounded common-sense inference tasks, outperforming other models by substantial margins.\n\n### Insights from Ablation Studies\n- **Impact of Pre-training Tasks**: \n  - Omitting the NSP task leads to a significant drop in performance, underscoring the importance of bidirectionality in training.\n\n- **Model Size**: \n  - Larger BERT models consistently achieve better accuracy, indicating that increased model size enhances performance.\n\n- **Importance of Extensive Pre-training**: \n  - BERT requires extensive pre-training (1 million steps) to achieve optimal performance, with a notable increase in accuracy observed when pre-training is extended.\n\n- **Comparison of Masking Strategies**: \n  - The MLM approach converges at a slower rate compared to left-to-right models but ultimately outperforms them in accuracy, highlighting its effectiveness in capturing contextual relationships.\n\n- **Challenges with Masking Strategies**: \n  - The study identifies limitations with the MASK strategy in feature-based approaches, particularly in named entity recognition (NER) tasks.\n\n- **Effectiveness of Mixed Masking Strategy**: \n  - BERT's mixed masking strategy enhances model performance, while reliance on random tokens for masking leads to suboptimal results.\n\n### Fine-tuning Approaches\n- **Feature-based vs. Fine-tuning**: \n  - The feature-based approach offers computational advantages and flexibility, performing competitively with state-of-the-art methods.\n\n### Hyperparameter Tuning\n- **Sensitivity to Hyperparameters**: \n  - Larger datasets exhibit less sensitivity to hyperparameter choices, indicating the necessity for exhaustive searches for optimal parameters.\n\n### Comparison with Other Models\n- **Architectural Differences**: \n  - BERT's bidirectional architecture and pre-training tasks significantly contribute to its performance improvements compared to models like OpenAI GPT and ELMo.\n\n### Conclusion\nBERT's architecture and training strategies signify a substantial advancement in natural language understanding"
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(output_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQLdu5yEzKBK"
      },
      "source": [
        "### **2. Summarizing with the 'Refine' Chain**\n",
        "\n",
        "This method involves **an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document**, asking the LLM to refine the output based on the new document.\n",
        "\n",
        "**Pros:** Can pull in more relevant context, and may be less lossy than MapReduceDocumentsChain.\n",
        "\n",
        "**Cons:** Requires many more calls to the LLM than StuffDocumentsChain. The calls are also NOT independent, meaning they cannot be paralleled like MapReduceDocumentsChain. There is also some potential dependencies on the ordering of the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fn91_ymzLjD",
        "outputId": "21498802-df79-4b5b-8f22-0f2e22250ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.8 s, sys: 212 ms, total: 2.01 s\n",
            "Wall time: 4min 48s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "chain = load_summarize_chain(llm,                        # loads a summarization chain using a language model thet we defined earlier.\n",
        "                             chain_type=\"refine\")        # and specifies the chain type as \"refine\"\n",
        "\n",
        "output_summary = chain.invoke(chunks)[\"output_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LNPt0qJzRX-",
        "outputId": "ac74e82b-f40d-4ca7-bdec-23b93294e3f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RefineDocumentsChain(initial_llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x78748bf02f50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x78748bf41930>, root_client=<openai.OpenAI object at 0x787494191660>, root_async_client=<openai.AsyncOpenAI object at 0x78748bf01bd0>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), refine_llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['existing_answer', 'text'], template=\"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x78748bf02f50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x78748bf41930>, root_client=<openai.OpenAI object at 0x787494191660>, root_async_client=<openai.AsyncOpenAI object at 0x78748bf01bd0>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text', initial_response_name='existing_answer')"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "JUugkXB0zVja",
        "outputId": "e0016823-b0df-44ad-d54d-ce014e5fa425"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. Unlike previous models that use unidirectional context, BERT pre-trains deep bidirectional representations from unlabeled text, allowing it to consider both left and right context. This design enables BERT to be fine-tuned with minimal additional architecture for various natural language processing tasks, achieving state-of-the-art results on eleven benchmarks, including significant improvements in GLUE, MultiNLI, and SQuAD tasks.\n",
              "\n",
              "BERT employs a masked language model (MLM) pre-training objective, which enhances its effectiveness for both sentence-level and token-level tasks by fusing left and right context. The MLM involves randomly masking a percentage of input tokens and predicting them, allowing the model to learn bidirectional representations. Specifically, during the masking procedure, 80% of the time a token is replaced with a [MASK] token, 10% of the time it is replaced with a random word, and 10% of the time it remains unchanged. This approach ensures that the model maintains a contextual representation of every input token. Additionally, BERT incorporates a \"next sentence prediction\" (NSP) task that trains the model to understand the relationship between sentence pairs, which is crucial for tasks like Question Answering (QA) and Natural Language Inference (NLI). The NSP task is designed to transfer all parameters to initialize end-task model parameters, unlike prior work that only transferred sentence embeddings.\n",
              "\n",
              "The model architecture is based on a multi-layer bidirectional Transformer encoder, with two primary sizes: BERTBASE and BERTLARGE, which differ in the number of layers, hidden size, and total parameters. The input representation can handle both single sentences and pairs of sentences, using WordPiece embeddings with a 30,000 token vocabulary. The input embeddings are the sum of token embeddings, segment embeddings, and position embeddings, with the first token of every sequence being a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.\n",
              "\n",
              "Fine-tuning BERT is straightforward due to the self-attention mechanism in the Transformer, allowing it to model various downstream tasks by simply plugging in task-specific inputs and outputs. This process is relatively inexpensive, with results replicable in a short time on standard hardware. The contributions of the paper include demonstrating the importance of bidirectional pre-training for language representations, showing that pre-trained representations reduce the need for heavily-engineered task-specific architectures, and advancing the state of the art for eleven NLP tasks.\n",
              "\n",
              "BERTBASE and BERTLARGE significantly outperform previous state-of-the-art models across all tasks, achieving average accuracy improvements of 4.5% and 7.0%, respectively, on the GLUE benchmark. For instance, BERTLARGE achieved an accuracy of 86.7% on MNLI, surpassing OpenAI GPT's 82.1%. In the SQuAD v1.1 task, BERT demonstrated superior performance, with BERTLARGE (Single) achieving an F1 score of 90.9, outperforming the top leaderboard systems, including an ensemble of nlnet and QANet. In SQuAD v2.0, BERT also showed a +5.1 F1 improvement over the previous best system, demonstrating its robustness in handling more complex question-answering scenarios. Additionally, BERTLARGE outperformed the authors’ baseline systems in the SWAG dataset by a significant margin.\n",
              "\n",
              "Ablation studies highlighted the importance of the NSP task, showing that removing it significantly degrades performance on tasks like QNLI, MNLI, and SQuAD. Further analysis indicated that larger models consistently lead to improved accuracy across various tasks, even those with limited training data, underscoring the effectiveness of BERT's architecture and pre-training strategy. Additional ablation studies examined the effect of the number of training steps and different masking procedures during MLM pre-training. Results showed that BERTBASE benefits from extensive pre-training, achieving higher accuracy with more training steps, and that the mixed masking strategy used during pre-training is robust, although certain strategies can negatively impact performance in feature-based approaches.\n",
              "\n",
              "The paper also explores the feature-based approach, demonstrating that BERT can be effectively utilized for tasks like Named Entity Recognition (NER) by extracting fixed features from the pre-trained model, which can be beneficial for certain tasks and computationally efficient. The code and pre-trained models are available at https://github.com/google-research/bert.\n",
              "\n",
              "Overall, BERT's architecture, which uniquely combines bidirectional context and a fine-tuning approach, sets it apart from previous models like OpenAI GPT and ELMo, leading to significant advancements in the field of natural language processing. The paper also notes that fine-tuning is typically fast and less sensitive to hyperparameter choices when large datasets (e.g., 100k+ labeled training examples) are used, allowing for an exhaustive search over parameters to optimize performance on development sets. Comparisons with ELMo and"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(output_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLJmnz9TVCRL"
      },
      "source": [
        "___\n",
        "\n",
        "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
        "\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}